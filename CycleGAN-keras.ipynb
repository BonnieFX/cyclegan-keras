{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras implementation of https://github.com/junyanz/CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, ZeroPadding2D, BatchNormalization, Input, Dropout\n",
    "from keras.layers import Conv2DTranspose, UpSampling2D, Activation, Add, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu\n",
    "from keras.initializers import RandomNormal\n",
    "from keras_contrib.layers.normalization import InstanceNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights initializations\n",
    "\n",
    "# for convolution kernel\n",
    "conv_init = RandomNormal(0, 0.02)\n",
    "# for batch normalization\n",
    "gamma_init = RandomNormal(1., 0.02) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(f, *a, **k):\n",
    "    return Conv2D(f, kernel_initializer = conv_init, *a, **k)\n",
    "def batchnorm():\n",
    "    return BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5,\n",
    "                                   gamma_initializer = gamma_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filters, size, stride=(2, 2), has_norm_layer=True, use_norm_instance=False,\n",
    "               has_activation_layer=True, use_leaky_relu=False, padding='same'):\n",
    "    x = conv2d(filters, (size, size), strides=stride, padding=padding)(x)\n",
    "    if has_norm_layer:\n",
    "        if not use_norm_instance:\n",
    "            x = batchnorm()(x)\n",
    "        else:\n",
    "            x = InstanceNormalization(axis=1)(x)\n",
    "    if has_activation_layer:\n",
    "        if not use_leaky_relu:\n",
    "            x = Activation('relu')(x)\n",
    "        else:\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def res_block(x, filters=256, use_dropout=False):\n",
    "    y = conv_block(x, filters, 3, (1, 1))\n",
    "    if use_dropout:\n",
    "        y = Dropout(0.5)(y)\n",
    "    y = conv_block(y, filters, 3, (1, 1), has_activation_layer=False)\n",
    "    return Add()([y, x])\n",
    "\n",
    "# decoder block\n",
    "def up_block(x, filters, size, use_conv_transpose=True, use_norm_instance=False):\n",
    "    if use_conv_transpose:\n",
    "        x = Conv2DTranspose(filters, kernel_size=size, strides=2, padding='same',\n",
    "                            use_bias=True if use_norm_instance else False,\n",
    "                            kernel_initializer=RandomNormal(0, 0.02))(x)\n",
    "        x = batchnorm()(x)\n",
    "        x = Activation('relu')(x)\n",
    "    else:\n",
    "        x = UpSampling2D()(x)\n",
    "        x = conv_block(x, filters, size, (1, 1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the PatchGAN discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_layer_discriminator(image_size=256, input_nc=3, ndf=64, hidden_layers=3):\n",
    "    \"\"\"\n",
    "        input_nc: input channels\n",
    "        ndf: filters of the first layer\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(image_size, image_size, input_nc))\n",
    "    x = inputs\n",
    "    \n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = conv_block(x, ndf, 4, has_norm_layer=False, use_leaky_relu=True, padding='valid')\n",
    "    \n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    for i in range(1, hidden_layers + 1):\n",
    "        nf = 2 ** i * ndf\n",
    "        x = conv_block(x, nf, 4, use_leaky_relu=True, padding='valid')\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        \n",
    "    x = conv2d(1, (4, 4), activation='sigmoid', strides=(1, 1))(x)\n",
    "    outputs = x\n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_generator(image_size=256, input_nc=3, res_blocks=6, use_conv_transpose=True):\n",
    "    inputs = Input(shape=(image_size, image_size, input_nc))\n",
    "    x = inputs\n",
    "    \n",
    "    x = conv_block(x, 64, 7, (1, 1))\n",
    "    x = conv_block(x, 128, 3, (2, 2))\n",
    "    x = conv_block(x, 256, 3, (2, 2))\n",
    "    \n",
    "    for i in range(res_blocks):\n",
    "        x = res_block(x)\n",
    "        \n",
    "    x = up_block(x, 128, 3, use_conv_transpose=use_conv_transpose)\n",
    "    x = up_block(x, 64, 3, use_conv_transpose=use_conv_transpose)\n",
    "    \n",
    "    x = conv2d(3, (7, 7), activation='tanh', strides=(1, 1) ,padding='same')(x)    \n",
    "    outputs = x\n",
    "    return Model(inputs=inputs, outputs=outputs), inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpath = '/home/lin/Downloads/weights-data/'\n",
    "image_size=256\n",
    "batch_size = 1\n",
    "input_nc = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD_A = n_layer_discriminator()\n",
    "netD_B = n_layer_discriminator()\n",
    "# netD_A.summary()\n",
    "# netD_B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_GAN(output, target, use_lsgan=True):\n",
    "    if use_lsgan:\n",
    "        diff = output-target\n",
    "        dims = list(range(1,K.ndim(diff)))\n",
    "        return K.expand_dims((K.mean(diff**2, dims)), 0)\n",
    "    else:\n",
    "        return K.mean(K.log(output+1e-12)*target+K.log(1-output+1e-12)*(1-target))\n",
    "    \n",
    "def criterion_cycle(rec, real):\n",
    "    diff = K.abs(rec-real)\n",
    "    dims = list(range(1,K.ndim(diff)))\n",
    "    return K.expand_dims((K.mean(diff, dims)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netG_loss(inputs, cycle_loss_weight=1.0):\n",
    "    netD_B_predict_fake, rec_A, real_A, netD_A_predict_fake, rec_B, real_B = inputs\n",
    "    \n",
    "    loss_G_A = criterion_GAN(netD_B_predict_fake, K.ones_like(netD_B_predict_fake))\n",
    "    loss_cyc_A = criterion_cycle(rec_A, real_A)\n",
    "    \n",
    "    loss_G_B = criterion_GAN(netD_A_predict_fake, K.ones_like(netD_A_predict_fake))\n",
    "    loss_cyc_B = criterion_cycle(rec_B, real_B)\n",
    "    \n",
    "    loss_G = loss_G_A + loss_G_B + cycle_loss_weight * (loss_cyc_A+loss_cyc_B)\n",
    "    return loss_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netD_loss(netD_predict):\n",
    "    netD_predict_real, netD_predict_fake = netD_predict\n",
    "    \n",
    "    netD_loss_real = criterion_GAN(netD_predict_real, K.ones_like(netD_predict_real))\n",
    "    netD_loss_fake = criterion_GAN(netD_predict_fake, K.zeros_like(netD_predict_fake))\n",
    "    \n",
    "    loss_netD= 0.5  *  (netD_loss_real + netD_loss_fake)\n",
    "    return loss_netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD_A = n_layer_discriminator()\n",
    "netD_B = n_layer_discriminator()\n",
    "# netD_A.summary()\n",
    "# netD_B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG_A, real_A, fake_B = resnet_generator(use_conv_transpose=True)\n",
    "netG_B, real_B, fake_A = resnet_generator(use_conv_transpose=True)\n",
    "# netG_A.summary()\n",
    "# netG_B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make generater train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD_B_predict_fake = netD_B(fake_B)\n",
    "rec_A= netG_B(fake_B)\n",
    "netD_A_predict_fake = netD_A(fake_A)\n",
    "rec_B = netG_A(fake_A)\n",
    "lambda_layer_inputs = [netD_B_predict_fake, rec_A, real_A, netD_A_predict_fake, rec_B, real_B]\n",
    "\n",
    "for l in netG_A.layers: \n",
    "    l.trainable=True\n",
    "for l in netG_B.layers: \n",
    "    l.trainable=True\n",
    "for l in netD_A.layers: \n",
    "    l.trainable=False\n",
    "for l in netD_B.layers: \n",
    "    l.trainable=False\n",
    "        \n",
    "netG_train_function = Model([real_A, real_B],Lambda(netG_loss)(lambda_layer_inputs))\n",
    "Adam(lr=2e-4, beta_1=0.5, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "netG_train_function.compile('adam', 'mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make discriminator A train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD_A_predict_real = netD_A(real_A)\n",
    "\n",
    "_fake_A = Input(shape=(image_size, image_size, input_nc))\n",
    "_netD_A_predict_fake = netD_A(_fake_A)\n",
    "\n",
    "for l in netG_A.layers: \n",
    "    l.trainable=False\n",
    "for l in netG_B.layers: \n",
    "    l.trainable=False\n",
    "for l in netD_A.layers: \n",
    "    l.trainable=True      \n",
    "for l in netD_B.layers: \n",
    "    l.trainable=False\n",
    "\n",
    "netD_A_train_function = Model([real_A, _fake_A], Lambda(netD_loss)([netD_A_predict_real, _netD_A_predict_fake]))\n",
    "netD_A_train_function.compile('adam', 'mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make discriminator B train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD_B_predict_real = netD_B(real_B)\n",
    "\n",
    "_fake_B = Input(shape=(image_size, image_size, input_nc))\n",
    "_netD_B_predict_fake = netD_B(_fake_B)\n",
    "\n",
    "for l in netG_A.layers: \n",
    "    l.trainable=False\n",
    "for l in netG_B.layers: \n",
    "    l.trainable=False\n",
    "for l in netD_B.layers: \n",
    "     l.trainable=True  \n",
    "for l in netD_A.layers: \n",
    "    l.trainable=False \n",
    "        \n",
    "netD_B_train_function= Model([real_B, _fake_B], Lambda(netD_loss)([netD_B_predict_real, _netD_B_predict_fake]))\n",
    "netD_B_train_function.compile('adam', 'mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "from PIL import Image\n",
    "from random import randint, shuffle\n",
    "\n",
    "def load_data(file_pattern):\n",
    "    return glob.glob(file_pattern)\n",
    "\n",
    "def read_image(img, loadsize=286, imagesize=256):\n",
    "    img = Image.open(img).convert('RGB')\n",
    "    img = img.resize((loadsize, loadsize), Image.BICUBIC)\n",
    "    img = np.array(img)\n",
    "    img = img.astype(np.float32)\n",
    "    img = (img-127.5) / 127.5\n",
    "    # random jitter\n",
    "    w_offset = h_offset = randint(0, max(0, loadsize - imagesize - 1))\n",
    "    img = img[h_offset:h_offset + imagesize,\n",
    "          w_offset:w_offset + imagesize, :]\n",
    "    # horizontal flip\n",
    "    if randint(0, 1):\n",
    "        img = img[:, ::-1]\n",
    "    return img\n",
    "\n",
    "def try_read_img(data, index):\n",
    "    try:\n",
    "        img = read_image(data[index])\n",
    "        return img\n",
    "    except:\n",
    "        try_read_img(data, index + 1)\n",
    "\n",
    "train_A = load_data('/home/lin/Downloads/m-cycle/trainA/*')\n",
    "train_B = load_data('/home/lin/Downloads/m-cycle/trainB/*')\n",
    "print(len(train_A))\n",
    "print(len(train_B))\n",
    "\n",
    "val_A = load_data('/home/lin/Downloads/m-cycle/testA/*')\n",
    "val_B = load_data('/home/lin/Downloads/m-cycle/testB/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(data, batch_size):\n",
    "    length = len(data)\n",
    "    epoch = i = 0\n",
    "    tmpsize = None   \n",
    "    \n",
    "    while True:\n",
    "        size = tmpsize if tmpsize else batch_size\n",
    "        if i+size > length:\n",
    "            shuffle(data)\n",
    "            i = 0\n",
    "            epoch+=1        \n",
    "        rtn = []\n",
    "        for j in range(i,i+size):\n",
    "            img = try_read_img(data, j)\n",
    "            rtn.append(img)\n",
    "                \n",
    "        i+=size\n",
    "        tmpsize = yield epoch, np.float32(rtn)\n",
    "\n",
    "def minibatchAB(dataA, dataB, batch_size):\n",
    "    batchA=minibatch(dataA, batch_size)\n",
    "    batchB=minibatch(dataB, batch_size)\n",
    "    tmpsize = None    \n",
    "    while True:\n",
    "        ep1, A = batchA.send(tmpsize)\n",
    "        ep2, B = batchB.send(tmpsize)\n",
    "        tmpsize = yield max(ep1, ep2), A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "def display_image(X, rows=1):\n",
    "    assert X.shape[0]%rows == 0\n",
    "    int_X = ((X*127.5+127.5).clip(0,255).astype('uint8'))\n",
    "    int_X = int_X.reshape(-1,image_size,image_size, 3)\n",
    "    int_X = int_X.reshape(rows, -1, image_size, image_size,3).swapaxes(1,2).reshape(rows*image_size,-1, 3)\n",
    "    pil_X = Image.fromarray(int_X)\n",
    "    t = str(round(time.time()))\n",
    "    pil_X.save(dpath+'results/'+ t, 'JPEG')\n",
    "    display(pil_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = minibatchAB(train_A, train_B, 6)\n",
    "\n",
    "_, A, B = next(train_batch)\n",
    "display_image(A)\n",
    "display_image(B)\n",
    "_, A, B = next(train_batch)\n",
    "display_image(A)\n",
    "display_image(B)\n",
    "del train_batch, A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch = minibatchAB(val_A, val_B, 6)\n",
    "\n",
    "_, A, B = next(val_batch)\n",
    "display_image(A)\n",
    "display_image(B)\n",
    "del val_batch, A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(netG_alpha, netG_beta, X):\n",
    "    real_input = X\n",
    "    fake_output = netG_alpha.predict(real_input)\n",
    "    rec_input = netG_beta.predict(fake_output)\n",
    "    outputs = [fake_output, rec_input]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_output(netG_alpha, netG_beta, X):\n",
    "    r = [get_output(netG_alpha, netG_beta, X[i:i+1]) for i in range(X.shape[0])]\n",
    "    r = np.array(r)\n",
    "    return r.swapaxes(0,1)[:,:,0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_generator_image(A,B, netG_alpha,  netG_beta):\n",
    "    assert A.shape==B.shape\n",
    "      \n",
    "    rA = get_combined_output(netG_alpha, netG_beta, A)\n",
    "    rB = get_combined_output(netG_beta, netG_alpha, B)\n",
    "    \n",
    "    arr = np.concatenate([A,B,rA[0],rB[0],rA[1],rB[1]])    \n",
    "    display_image(arr, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generater_function(netG):\n",
    "    real_input = netG.inputs[0]\n",
    "    fake_output = netG.outputs[0]\n",
    "    function = K.function([real_input], [fake_output])\n",
    "    return function\n",
    "\n",
    "netG_A_function = get_generater_function(netG_A)\n",
    "netG_B_function = get_generater_function(netG_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('error', Image.DecompressionBombWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "time_start = time.time()\n",
    "how_many_epochs = 2\n",
    "iteration_count = 410000\n",
    "epoch_count = 0\n",
    "batch_size = 1\n",
    "display_freq = 1000\n",
    "val_batch = minibatchAB(val_A, val_B, batch_size=4)\n",
    "train_batch = minibatchAB(train_A, train_B, batch_size)\n",
    "\n",
    "while epoch_count < how_many_epochs: \n",
    "    target_label = np.zeros((batch_size, 1))\n",
    "    epoch_count, A, B = next(train_batch)\n",
    "\n",
    "    try:\n",
    "        _fake_B = netG_A_function([A])[0]\n",
    "        _fake_A = netG_B_function([B])[0]\n",
    "    except:\n",
    "        epoch_count, A, B = next(train_batch)\n",
    "        _fake_B = netG_A_function([A])[0]\n",
    "        _fake_A = netG_B_function([B])[0]\n",
    "        \n",
    "\n",
    "    netG_train_function.train_on_batch([A, B], target_label)\n",
    "    \n",
    "    netD_B_train_function.train_on_batch([B, _fake_B], target_label)\n",
    "    netD_A_train_function.train_on_batch([A, _fake_A], target_label)\n",
    "    \n",
    "    iteration_count+=1\n",
    "    \n",
    "    if iteration_count%display_freq==0:\n",
    "        clear_output()\n",
    "        traintime =  (time.time()-time_start)/iteration_count\n",
    "        print('epoch_count: {}  iter_count: {}  timecost/iter: {}s'.format(epoch_count, iteration_count, traintime))\n",
    "        _, val_A, val_B = next(val_batch)\n",
    "        show_generator_image(val_A,val_B, netG_A, netG_B)\n",
    "        save_name = dpath + '{}' + str(iteration_count) + '.h5'\n",
    "        \n",
    "        netG_A.save_weights(save_name.format('tf_GA_weights'))\n",
    "        netG_B.save_weights(save_name.format('tf_GB_weights'))\n",
    "        netD_A.save_weights(save_name.format('tf_DA_weights'))\n",
    "        netD_B.save_weights(save_name.format('tf_DB_weights'))\n",
    "        netG_train_function.save_weights(save_name.format('tf_G_train_weights'))\n",
    "        netD_A_train_function.save_weights(save_name.format('tf_D_A_train_weights'))\n",
    "        netD_B_train_function.save_weights(save_name.format('tf_D_B_train_weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.learning_phase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = dpath + '{}' + '200000.h5'\n",
    "netG_A.load_weights(save_name.format('tf_GA_weights'))\n",
    "netG_B.load_weights(save_name.format('tf_GB_weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch = minibatchAB(val_A, val_B, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run batch normalization layer in inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,A, B = next(val_batch)\n",
    "save_name = dpath + '{}' + '400000.h5'\n",
    "netG_A.load_weights(save_name.format('tf_GA_weights'))\n",
    "netG_B.load_weights(save_name.format('tf_GB_weights'))\n",
    "show_generator_image(A,B, netG_A, netG_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run batch normalization layer in training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cycle_generater (netG_alpha, netG_beta):\n",
    "    real_input = netG_alpha.inputs[0]\n",
    "    fake_output = netG_alpha.outputs[0]\n",
    "    rec_input = netG_beta([fake_output])\n",
    "    generater = K.function([real_input, K.learning_phase()], [fake_output, rec_input])\n",
    "    return generater\n",
    "\n",
    "cycleA_generater = get_cycle_generater(netG_A, netG_B)\n",
    "cycleB_generater = get_cycle_generater(netG_B, netG_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_netG(A,B):\n",
    "    assert A.shape==B.shape\n",
    "    def G(generater, X):\n",
    "        r = np.array([generater([X[i:i+1], 1]) for i in range(X.shape[0])])\n",
    "        return r.swapaxes(0,1)[:,:,0]        \n",
    "    rA = G(cycleA_generater, A)\n",
    "    rB = G(cycleB_generater, B)\n",
    "    arr = np.concatenate([A,B,rA[0],rB[0],rA[1],rB[1]])\n",
    "    display_image(arr, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,A, B = next(val_batch)\n",
    "show_netG(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
